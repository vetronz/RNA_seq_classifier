library(AmesHousing)
swiss <- datasets::swiss
swiss
dim(swiss)
x <- model.matrix(Fertility~., swiss)[,-1]
x
y <- swiss$Fertility
lambda <- 10^seq(10, -2, length = 100)
lambda
#create test and training sets
library(glmnet)
set.seed(489)
train = sample(1:nrow(x), nrow(x)/2)
test = (-train)
ytest = y[test]
#OLS
swisslm <- lm(Fertility~., data = swiss)
coef(swisslm)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = lambda)
predict(ridge.mod, s = 0, exact = T, type = 'coefficients')[1:6,]
ridge.mod <- glmnet(x, y, alpha = 0, lambda = lambda)
predict(ridge.mod, s = 0, exact = T, type = 'coefficients')[1:6,]
#ridge
ridge.mod <- glmnet(x, y, alpha = 0, lambda = lambda)
predict(ridge.mod, s = 0, exact = T, type = 'coefficients')[1:6,]
predict(ridge.mod, s = 0, type = 'coefficients')[1:6,]
coef(swisslm)
swisslm <- lm(Fertility~., data = swiss, subset = train)
ridge.mod <- glmnet(x[train,], y[train], alpha = 0, lambda = lambda)
#find the best lambda from our list via cross-validation
cv.out <- cv.glmnet(x[train,], y[train], alpha = 0)
bestlam <- cv.out$lambda.min
#make predictions
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test,])
s.pred <- predict(swisslm, newdata = swiss[test,])
#check MSE
mean((s.pred-ytest)^2)
mean((ridge.pred-ytest)^2)
lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = lambda)
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test,])
mean((lasso.pred-ytest)^2)
lasso.coef  <- predict(lasso.mod, type = 'coefficients', s = bestlam)[1:6,]
lasso.coef
lasso.coef  <- predict(lasso.mod, type = 'coefficients', s = bestlam)[1:6,]
mean((lasso.pred-ytest)^2)
getwd()
index <- sample(1:nrow(swiss), round(nrow(swiss) * 0.7))
index
train_1 <- swiss[index, ]
test_1  <- swiss[-index, ]
train_1
test_1
ytest = y[test]
ytest
y
train
set.seed(489)
train = sample(1:nrow(x), nrow(x)/2)
test = (-train)
ytest = y[test]
train
test
train_2
train_1
swiss[index, ][,-1]
test_1  <- swiss[-index, ]$Fertility
test_1
index
length(index)
length(swiss)
length(swiss[[1]])
47-33
length(test_1)
train_1
test_1
#OLS
swisslm <- lm(Fertility~., data = swiss)
coef(swisslm)
length(x)
x
dim(xx)
dim(x)
dim(swiss)
#ridge
ridge.mod <- glmnet(x, y, alpha = 0, lambda = lambda)
predict(ridge.mod, s = 0, type = 'coefficients')[1:6,]
lambda
pred <- predict(ridge.mod, s = 0, type = 'coefficients')[1:6,]
dim(pred)
length(pred)
?sample
seq(1:10)
a<-seq(1:10)
sample(a)
sample(10, 5)
sample(10, 5)
sample(10, 5)
length(train)
length(swiss[[1]])
train = sample(1:nrow(x), round(nrow(x)*0.7))
train
length(train)
33/47
#create test and training sets
set.seed(489)
train = sample(1:nrow(x), round(nrow(x)*0.7))
test = (-train)
ytest = y[test]
#OLS
swisslm <- lm(Fertility~., data = swiss)
coef(swisslm)
#ridge
ridge.mod <- glmnet(x, y, alpha = 0, lambda = lambda)
predict(ridge.mod, s = 0, type = 'coefficients')[1:6,]
predict(ridge.mod, s = 0, type = 'coefficients')
swisslm <- lm(Fertility~., data = swiss, subset = train)
coef(swisslm)
x
x[1,]
x[1:2,]
x[1:3,]
x[c(1,3),]
ridge.mod <- glmnet(x[train,], y[train], alpha = 0, lambda = lambda)
ridge.mod
#find the best lambda from our list via cross-validation
cv.out <- cv.glmnet(x[train,], y[train], alpha = 0)
cv.out
cv.out$lambda.min
swiss[test,]
x[test,]
#make predictions
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test,])
s.pred <- predict(swisslm, newdata = swiss[test,])
#check MSE
mean((s.pred-ytest)^2)
mean((ridge.pred-ytest)^2)
s.pred <- predict(swisslm, newdata = x[test,])
as.data.frame(x)
as.data.frame(x)[1]
as.data.frame(x)[1:2,]
s.pred <- predict(swisslm, newdata = as.data.frame(x)[test,])
#make predictions
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test,])
s.pred <- predict(swisslm, newdata = as.data.frame(x)[test,])
#check MSE
mean((s.pred-ytest)^2)
mean((ridge.pred-ytest)^2)
#make predictions
ridge.pred <- predict(ridge.mod, s = bestlam, newdata = x[test,])
s.pred <- predict(swisslm, newx = as.data.frame(x)[test,])
#make predictions
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test,])
s.pred <- predict(swisslm, newx = as.data.frame(x)[test,])
#check MSE
mean((s.pred-ytest)^2)
mean((ridge.pred-ytest)^2)
s.pred <- predict(swisslm, newx = as.data.frame(x)[test,])
#check MSE
mean((s.pred-ytest)^2)
s.pred <- predict(swisslm, newdata = as.data.frame(x)[test,])
#check MSE
mean((s.pred-ytest)^2)
mean((ridge.pred-ytest)^2)
getwd()
x
y
plot(y, length(y))
length(y)
y
hist(y)
y.cat <- y > 80
y.cat
train
train = sample(1:nrow(x), round(nrow(x)*0.7))
test = (-train)
ytest = y.cat[test]
ytest
#OLS
swisslm <- lm(Fertility~., data = swiss)
coef(swisslm)
swisslm <- lm(Fertility~., data = x)
swisslm <- lm(Fertility~., data = as.data.frame(x))
coef(swisslm)
#OLS
swisslm <- lm(Fertility~., data = swiss)
coef(swisslm)
#make predictions
s.pred <- predict(swisslm, newdata = as.data.frame(x)[test,])
#check MSE
mean((s.pred-ytest)^2)
mean((ridge.pred-ytest)^2)
library(glmnet)   # implementing regularized regression approaches
swiss <- datasets::swiss
swiss
dim(swiss)
x <- model.matrix(Fertility~., swiss)[,-1]
y <- swiss$Fertility
y.cat <- y > 80
lambda <- 10^seq(10, -2, length = 100)
#create test and training sets
set.seed(489)
train = sample(1:nrow(x), round(nrow(x)*0.7))
test = (-train)
ytest = y.cat[test]
#OLS
swisslm <- lm(Fertility~., data = swiss)
#create test and training sets
set.seed(489)
train = sample(1:nrow(x), round(nrow(x)*0.7))
test = (-train)
ytest = y[test]
y.cat.test = y.cat[test]
#OLS
swisslm <- lm(Fertility~., data = swiss)
coef(swisslm)
#ridge
ridge.mod <- glmnet(x, y, alpha = 0, lambda = lambda)
predict(ridge.mod, s = 0, type = 'coefficients')
swisslm <- lm(Fertility~., data = swiss, subset = train)
coef(swisslm)
ridge.mod <- glmnet(x[train,], y[train], alpha = 0, lambda = lambda)
#find the best lambda from our list via cross-validation
cv.out <- cv.glmnet(x[train,], y[train], alpha = 0)
bestlam <- cv.out$lambda.min
#make predictions
s.pred <- predict(swisslm, newdata = as.data.frame(x)[test,])
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test,])
#check MSE
mean((s.pred-ytest)^2)
mean((ridge.pred-ytest)^2)
caret::varImp(swisslm)
caret::varImp(ridge.mod)
anova(swisslm, ridge.mod, test = "Chisq")
s.pred
ridge.pred
ridge.pred[,]
ridge.pred[1,]
ridge.pred[,1]
# classification
s.pred
# classification
s.class <- s.pred > 80
r.class <- ridge.pred > 80
s.class == r.class
y.cat.test
s.class == r.class == y.cat.test
s.class == y.cat.test
s.class == r.class
y.cat.test
table(y.cat.test, s.class)
table(y.cat.test, s.class) %>% prop.table
library(dplyr)
table(y.cat.test, s.class) %>% prop.table
table(y.cat.test, s.class) %>% prop.table()
table(y.cat.test, s.class) %>% prop.table() %>% round(2)
table(y.cat.test, s.class) %>% prop.table() %>% round(3)
table(y.cat.test, r.class) %>% prop.table() %>% round(3)
list(
table(y.cat.test, s.class) %>% prop.table() %>% round(3)
table(y.cat.test, r.class) %>% prop.table() %>% round(3)
)
list(
table(y.cat.test, s.class) %>% prop.table() %>% round(3),
table(y.cat.test, r.class) %>% prop.table() %>% round(3)
)
library(ROCR)
library(glmnet)   # implementing regularized regression approaches
library(dplyr)
swiss <- datasets::swiss
swiss
dim(swiss)
x <- model.matrix(Fertility~., swiss)[,-1]
y <- swiss$Fertility
y.cat <- y > 80
lambda <- 10^seq(10, -2, length = 100)
#create test and training sets
set.seed(489)
train = sample(1:nrow(x), round(nrow(x)*0.7))
test = (-train)
ytest = y[test]
y.cat.test = y.cat[test]
#OLS
swisslm <- lm(Fertility~., data = swiss)
coef(swisslm)
#ridge
ridge.mod <- glmnet(x, y, alpha = 0, lambda = lambda)
predict(ridge.mod, s = 0, type = 'coefficients')
swisslm <- lm(Fertility~., data = swiss, subset = train)
coef(swisslm)
ridge.mod <- glmnet(x[train,], y[train], alpha = 0, lambda = lambda)
#find the best lambda from our list via cross-validation
cv.out <- cv.glmnet(x[train,], y[train], alpha = 0)
bestlam <- cv.out$lambda.min
#make predictions
s.pred <- predict(swisslm, newdata = as.data.frame(x)[test,])
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test,])
## MODEL EVALUATION
#check MSE
mean((s.pred-ytest)^2)
mean((ridge.pred-ytest)^2)
# classification
s.class <- s.pred > 80
r.class <- ridge.pred > 80
s.class == r.class
list(
table(y.cat.test, s.class) %>% prop.table() %>% round(3),
table(y.cat.test, r.class) %>% prop.table() %>% round(3)
)
par(mfrow=c(1, 2))
s.class
length(s.class)
y
ytest
y.test.cat
y.test.cat
y.cat.test
length(y.cat.test)
prediction(s.class, y.cat.test) %>%
performance(measure = "tpr", x.measure = "fpr") %>%
plot()
library(ROCR)
prediction(s.class, y.cat.test) %>%
performance(measure = "tpr", x.measure = "fpr") %>%
plot()
prediction(s.pred, y.cat.test) %>%
performance(measure = "tpr", x.measure = "fpr") %>%
plot()
prediction(ridge.pred, y.cat.test) %>%
performance(measure = "tpr", x.measure = "fpr") %>%
plot()
# model 1 AUC
prediction(s.pred, y.cat.test) %>%
performance(measure = "auc") %>%
.@y.values
# model 1 AUC
prediction(ridge.pred, y.cat.test) %>%
performance(measure = "auc") %>%
.@y.values
library(glmnet)   # implementing regularized regression approaches
library(dplyr)
library(ROCR)
swiss <- datasets::swiss
swiss
dim(swiss)
x <- model.matrix(Fertility~., swiss)[,-1]
y <- swiss$Fertility
y.cat <- y > 80
lambda <- 10^seq(10, -2, length = 100)
#create test and training sets
set.seed(489)
train = sample(1:nrow(x), round(nrow(x)*0.7))
test = (-train)
ytest = y[test]
y.cat.test = y.cat[test]
#OLS
swisslm <- lm(Fertility~., data = swiss)
coef(swisslm)
#ridge
ridge.mod <- glmnet(x, y, alpha = 0, lambda = lambda)
predict(ridge.mod, s = 0, type = 'coefficients')
swisslm <- lm(Fertility~., data = swiss, subset = train)
coef(swisslm)
ridge.mod <- glmnet(x[train,], y[train], alpha = 0, lambda = lambda)
#find the best lambda from our list via cross-validation
cv.out <- cv.glmnet(x[train,], y[train], alpha = 0)
bestlam <- cv.out$lambda.min
#make predictions
s.pred <- predict(swisslm, newdata = as.data.frame(x)[test,])
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test,])
## MODEL EVALUATION
#check MSE
mean((s.pred-ytest)^2)
mean((ridge.pred-ytest)^2)
# classification
s.class <- s.pred > 80
r.class <- ridge.pred > 80
s.class == r.class
list(
table(y.cat.test, s.class) %>% prop.table() %>% round(3),
table(y.cat.test, r.class) %>% prop.table() %>% round(3)
)
par(mfrow=c(1, 2))
prediction(s.pred, y.cat.test) %>%
performance(measure = "tpr", x.measure = "fpr") %>%
plot()
prediction(ridge.pred, y.cat.test) %>%
performance(measure = "tpr", x.measure = "fpr") %>%
plot()
caret::varImp(swisslm)
# model 1 AUC
prediction(s.pred, y.cat.test) %>%
performance(measure = "auc") %>%
.@y.values
# model 1 AUC
prediction(ridge.pred, y.cat.test) %>%
performance(measure = "auc") %>%
.@y.values
# lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = lambda)
# lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = lambda)
# lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test,])
# lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = lambda)
# lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test,])
# mean((lasso.pred-ytest)^2)
# lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = lambda)
# lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test,])
# mean((lasso.pred-ytest)^2)
#
# lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = lambda)
# lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test,])
# mean((lasso.pred-ytest)^2)
#
# lasso.coef  <- predict(lasso.mod, type = 'coefficients', s = bestlam)[1:6,]
# lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = lambda)
# lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test,])
# mean((lasso.pred-ytest)^2)
#
# lasso.coef  <- predict(lasso.mod, type = 'coefficients', s = bestlam)[1:6,]
# lasso.coef
# lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = lambda)
# lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test,])
# mean((lasso.pred-ytest)^2)
#
# lasso.coef  <- predict(lasso.mod, type = 'coefficients', s = bestlam)[1:6,]
# lasso.coef
# lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = lambda)
# lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test,])
# mean((lasso.pred-ytest)^2)
#
# lasso.coef  <- predict(lasso.mod, type = 'coefficients', s = bestlam)[1:6,]
# lasso.coef
# lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = lambda)
# lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test,])
# mean((lasso.pred-ytest)^2)
#
# lasso.coef  <- predict(lasso.mod, type = 'coefficients', s = bestlam)[1:6,]
# lasso.coef
# lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = lambda)
# lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test,])
# mean((lasso.pred-ytest)^2)
#
# lasso.coef  <- predict(lasso.mod, type = 'coefficients', s = bestlam)[1:6,]
# lasso.coef
# lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = lambda)
# lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test,])
# mean((lasso.pred-ytest)^2)
#
# lasso.coef  <- predict(lasso.mod, type = 'coefficients', s = bestlam)[1:6,]
# lasso.coef
